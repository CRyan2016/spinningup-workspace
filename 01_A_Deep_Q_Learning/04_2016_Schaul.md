# Title: Prioritized Experience Replay

# Author: Schaul et al (2016) (DeepMind)

#### General Content:

Authors extend the ER training scheme by introducing two key features: Stochastic prioritization (prioritize high TD error transitions - aggressiveness) and importance sampling (reweighting/grad magnitude reduction introduces implicit learning rule - robustness).

#### Key take-away for own work:

Replay memory = two part construct: Memory (which experiences should be stored) and Computation (which experiences should be used to estimate a gradient). PER addresses the computation aspect by TD-error weighting the sampling probs of the tuples. There has not been work done (?) on refining which tuples to keep in the buffer. Need for smarter forgetting.

#### Keypoints:

* Utility of ER: (1) Order - Decorrelate the sequentially generated samples to make iid SGD assumption plausible. (2) Frequency - Sliding memory combats rapid forgetting of rare experiences

* Schmidhuber 1991 - Some experiences become only valuable to the learning process later on after some skills were acquired

* Relationship to planning "prioritized sweeping" in model-based context. Here model-free. Hinton (2007) had also similar idea in supervised learning context

* Here: Replay transitions more freq with hight expected learning progress - magnitude TD error. Make stochastic to combat loss of diversity.
    * Two weighting schemes: Rank based and proportional to TD (both with temperature)
    * Introduces bias: Combat/Correct via importance sampling

* Neuro motivation:
    1. Seqs associated with rewards are replayed more frequently (see listed refs)
    2. Experiences with high magnitude TD error are replayed more often

* Motivating example: Greedy prioritization - always replay largest TD experience and once the new transition.
    * Problem: Only update TD errors of replayed transitions. Low error on first visit transitions might not be replayed at all (before rotating out). Also sensitive to noise spikes.
    * Solution: Stochasticity + temperature that allows to walk on spectrum between greedy and uniform (alpha=0)

* Implementing rank-based version of stoch prioritization: Rank of transition i - sorting of memory according to magnitude of TD error
    * Approximate CDF with k piecewise linear function segments of equal prob
    * Boundaries can be precomputed - change only when N (memory size) or alpha change
    * Sample segment and then uniformly sample transitions within segment
    * Minibatch size = k: sample exactly one transition from each segment - stratified sampling that balances batches out
    * Proportional variant requires sum-tree data structure
    * In practice: rank-based and proportional PER perform the same

* Biased introduced by uncontrolled change in sampling distribution - solve by reweighting the update TD error with form of inverse sampling probability. The higher the sampling rate the more the update magnitude is going to be scaled down.
    * Always scale update down by x 1/max w
    * Annealing of the beta exponent to 1 - unbiased nature of updates is most important near the convergence.
    * Correction allows algo to "follow the curvature of highly non-linear optimization landscapes because Taylor expansion is constantly re-approximated".

* Also pick smaller learning rate - Why necessary if we do importance sampling with error reweighting?

* Update only every 4 transitions with batchsize 32

#### Questions/Critical Observations:

* "A transition is the atomic unit of interaction in RL"

* Problem with using TD as learning progress measure = noise in the rewards - not learnable!

* Can this notion of surprising transitions be somehow related to MMN and Bayesian surprise?

* Still dont really understand how importance sampling corrects for the bias of non-uniform sampling. How to make sure that the scaling is actually combating the introduced bias? Read blogs?

* Everything seems very hacky. Still with intuition (i.e. beta annealing) but without any guarantees except for the performance.

* Interesting observation/hypothesis: top layer learns value given representation. bottom layers learn representations. Transitions with already good representation will quickly reduce error - afterwards replayed less. Which leads to foces with poor representations!
