# Title: Rainbow: Combining Improvements in DRL

# Author: Hessel et al (2017) (DeepMind)

#### General Content:

Authors combine 6 complementary extensions to the DQN framework and create new SOTA. They find in ablation study that Prioritized ER and multi-step rewards provide the most crucial performance boost. Furthermore, they highlight the interplay between the different extensions while holding the computation per update semi-constant. Not really innovative but computations only really on Google level feasible. Stack as follows: DQN + Multi-Step + Distributional + Double Q + PER + Dueling + Noisy Net


#### Key take-away for own work:

Have to read up on distributional (alternative kl loss) and noisy net work (exploration via learned stochasticity in ultimate layer). Sometimes pure computational power can get you a publication.

#### Keypoints:

* Forward-view Multi-step Bootstrap Targets (A3C): Shift bias-variance tradeoff (bias reduction? via more reward in target) - faster propagation of newly observed rewards.

* Noisy nets: Stochastic network layers for exploration - combine deterministic and noisy stream - combine both with separate weights - allow the network to learn amount of exploration needed in which part of space. = State-conditional exploration with form of self-annealing.

* Distributional Q Learning: Learn categorical distribution over discounted returns - instead of only estimating the mean
    * Prob mass placed on discrete support z with N_atoms num categories
    * parametrized distribution chosen to closely match actual return distribution - learnable since Bellman also applies - write down form of KL loss to minimize
    * Take softmax on final layer with N_atoms x N_actions outputs

* Episodic games <=> gamma=0 after termination. Think more generally about time-dependent discount factors

* PER: New transitions inserted with max priority - bias towards recent transitions

* Integrated agent: DQN + Multi-Step + Distributional + Double Q + PER + Dueling + Noisy Net
    * Dueling extension: Simply combine streams as before and then take the softmax
    * Noisy net extension: factorized Gaussian noise to reduce number of independent noise variables - problem with noise sampling in GPU tensorflow

* Tuning of hyperparams:
    * Tuned most sensititve hyperparams by manual coordinate descent.
    * DQN: No updates during first 200K frames to ensure sufficiently uncorrelated updates. With PER only around 80K needed.
    * Noisy net hyperparam on variance of noise vars - no epsilon exploration needed
    * Found Adam to be less sensitive than RMSprop
    * Found Distributional DQN to be robust to choice of PER hyperparams
    * n=3 for multistep updates

#### Questions/Critical Observations:

* Multi-step targets: Always fix single n? Why not vary across the learning process?

* Try symmetric JS divergence as objective for distributional DQN
