# Title: Implicit Quantile Networks for Distributional RL

# Author: Dabney et al (2018) (DeepMind)

#### General Content:
Authors extend the discrete set of quantiles in the QR-DQN setup towards sampling from a uniform distribution between 0, 1. This allows to approximate the full quantile function of the state-action distribution. By controlling and reshaping the quantile distribution sampled from during training, one can control the risk-sensitivity of the resulting policy. This can help in exploration as well as nudge agents to survive longer. When tested on ATARI, the agent outperforms QR-DQN.

#### Key take-away for own work:
Authors combine ideas from economic decision theory in order to adapt the risk sensitivity of the agent - super cool idea! They also shift the amount of samples required towards the computation required for a learning update. Less samples <-> More different quantile updates.

#### Keypoints:

* Key elements of distributional RL: parametrized return distribution, distance metric
    * Categorical DQN: Categorical distribution + cross-entropy loss with Cramer-minimizing projection
    * QR-DQN: Uniform mixture of diracs whose locations are ajusted using quantile regression. Automatically adapts return quantiles to min Wasserstein distance between Bellman updated and current return distributions
    * Here: Learn a continuous map from probabilities to returns!

* Benefits of modeling full quantile function:
    1. Approx error if distribution is no longer controlled by number of quantiles output by net but by size of the network itself and the training!
    2. IQN can use several samples per update and thereby refine estimates of multiple quantiles at same time!
    3. Implicit representation of return distribution - allows to expand class of policies to take advantage of learned distribution - incorporate arbitrary risk measures.

* Risk = Uncertainty over possible outcomes
    * sensitive policies = depend upon more than the mean of the outcomes
    * intrinsic u = captured by return distribution
    * parametric u = uncertainty over the value estimate - e.g. using Bayesian methods

* Expected utility theory: If a decision policy is consistent with the 4 axioms of choice then the policy behaves as though it is maximizing the expected value of some utility function
    * Yaari - dual theort of choice - max of distorted expectation - distortion risk measure
    * Two equicalent approaches: expectation of utility or reweighting of distribution and computation of expectation under distortion measure.
    * cumulative prospect theory, conditional value at risk

* Implicit Quantile Networks:
    * Deterministic parametric function trained to reparametrize samples from base distribution e.g. Uniform to respective quantile values of a target distribution
    * Use different different distortion risk measures mapping taus to constructed weighted versions
    which lead to different policies used to construct the sampled TD error
    * UVFA interpretation = train on many different goals/quantiles
    * Approx of Z = Value MLP (merging of conv stack & quantile sample embedding)
    * Merging given by elementwise product
    * Experiment with different no of samples for quantiles of transition value distribution and bootstrapped distribution - seems to saturate after 8 but still can benefit from more samples!
    * But also single sample appears to lead to improvements!

* ATARI experiments - show that improvements mainly result from games which were hard for classical DQN!

#### Questions/Critical Observations:

* Is it possible to do something similar by sampling from different prioritized distributions? And use different IS corrections?
* Would be nice to have a better and deeper understanding of what this actually does with the behavior of the agents - until now: very ad-hock!
