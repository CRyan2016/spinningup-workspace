# Title: Non-Delusional Q-Learning and Value Iteration

# Author: Lu et al. (2018 - NeuRIPS best paper award)

#### General Content:
Authors show that there is an additional source of error in Q-L which arises when the approximation limits the class of expressible greedy policies. Globally uncoordinated action choices lead to inconsistent and even conflicting updates - which can lead to bias as well as divergence. Authors propose local backup rule that ensures consistency by the help of a set of policy consistent constraints. But requires an oracle! Propose some practical heuristics.

#### Key take-away for own work:
Paper is really hard to use. Oracle as well as non-classical optimization/objective.

#### Keypoints:

* Approximate Q-Learning suffers from delusional bias - updates are based on mutually inconsistent values. Arises because Q-update for s,a is based on max value extimate over all actions at next state
    * Ignores fact that actions considered might not be jointly realizable given set of admissable policies derived from approximator
    * "Unconstrained" updates = Q-Learning backs up values based on action choices that greedy policy class cannot realize
    * Cannot be overcome by more expressive approximators or more training

* Introduce policy-consistent backups <-> temporal consistency
    * New operator: Dont simply backup single future value at each s-a pair. Instead backsup set of candidate values each with associated set of policy commitments that justify it
    * Complement existing Q-L. with explicit constraints on policies consistent with generated values and use values to select policies from admissable class.
    * No of info sets is bounded polynomially when greedy policy class has finite VC-dimension - algo has polynomial-time iteration complexity in tabular case.

* Heuristics for imposing policy consisitency:
    * consistent backups can cause info sets to proliferate - search heuristics that focus attention on promising info sets

* If no policy in admissible class can jointly express all past (implicit) action selections, backed-up values do not correspond to Q-values that can be achieved by any expressible policy. Applies to any approx class with finite capacity
    * Not due to approx error itself but inability of Q-L, to find value of optimal **representable policy**

* Consequences delusion:
    1. Divergence: Correction restores convergence
    2. Discounting paradox: training on different gamma can lead to better test performance on gamma defining MDP
    3. Approx DP: Full state Bellman backups - but: Like Q-L applies max operator independently at successor state when computing expected next state values
    4. Batch Q: Specific nonrandom batching schemes can cause greater degrees of delusion!

* Unification of value and policy based methods via notion of policy search with consistency constraint!

* Practical heuristics: Combat proliferation of info stes - mmultiple regressors, locally consistent data, consistent labelling

#### Questions/Critical Observations:

* Look deeper into math! How would you actually go about implementing this?
