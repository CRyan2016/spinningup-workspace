# Title: Deterministic Policy Gradient Algorithms

# Author: David Silver et al (2014) (DeepMind)

#### General Content:
Introduce a deterministic PG algorithm with continuous actions = expected gradient of action-value function. Exploration is done via off-policy AC algo. Furthermore, they derive conditions for a compatible function approximator in deterministic case that does not introduce additional bias from the critic. Previously believed that PG does not exists for deterministic policies. Highly applicable to cases were of continuous control where we can't inject noise into the controller.


#### Key take-away for own work:
Work on which Lillicrap DDPG rests - simply extend with deep function approximations - form DQN for continuous action spaces.

#### Keypoints:

* Difference stochastic vs deterministic: Integral over state + action space vs only over state space. Stochastic PG requires many more samples!

* Notion of compatible function approx for dpg = make sure approx does not bias the policy gradient.

* Policy gradient observation: Despite fact that state distribution does depend on params, the PG does not on gradient of the state distribution.
    * Difficulty: In its purest form requires on-policy state-value function - in practice use estimate, i.e. critic learned via policy evaluation!
    * Might introduce bias - not the case if compatible function approx is used. Two criteria: i. weights dot product with policy jacobian - linear in features of stochastic policy, ii. weights chosen to min MSE between approx and true state-value fct. - solution to regression problem
    * Condition ii. usually relaxed to policy eval algorithm - more efficient estimation
    * Off-policy estimation of PG: Use importance weights (ratio behavior/target policy) to correct for off-policy bias

* In general: Policy iteration = Policy eval + Policy improvement - greedy improvement step hard (requires global max at every step) in continous case!

* DPG Idea: Instead move policy in direction of gradient of Q rather than global max of Q

* Show that deterministic PG is a limiting case of the stochastic PG. Theoretical result for variance going to 0 in limit. Allows to generalize multiple other approaches e.g. natural gradients, ac, batch/episodic methods to DPG.

* On-policy + deterministic PG only makes sense if stochasticity in env is sufficient enough to learn!

* Off-policy deterministic AC: learn deterministic target policy from trajectories of arbitrary behavior policy. - modify performance objective to be value fct of target policy averaged over state distr of behavior policy.

#### Questions/Critical Observations:

* Work through the appendix derivation!
* How do they get rid of the state distribution?
