# Title: Asynchronous Methods for DRL

# Author: Mnih et al (2016) (DeepMind)

#### General Content:
Introduce a general and efficient parallel actor-learners paradigm which uses async gradient descent updates and trains on CPUs (relaxing hardware constraints). Thereby they are able to substitute the ER buffer needed for off-policy learning with async executing multiple agents in parallel. The variety of experiences allows to decorrelate and diversify the training data into a more stationary process. Furthermore, this easily translates into on-policy policy gradient algorithms (Sarsa, n-step returns, AC methods).


#### Key take-away for own work:
ER is not necessary to stabilize the non-stationarity of the incoming transitions. Strong CPUs might be more useful than GPUs to acquire large amounts of training data in parallel to replace buffer and increase efficiency via n-step returns.


#### Keypoints:

* Nair et al 2015: Each process requires actor, own copy of env, own copy of memory, and learner that samples data and computes gradients. Gradients are async send to central param server which updates central copy of model that is spawned to actors in fixed interval.

* One-step vs N-step updates:
    * One-step: Values of other state action pairs than current update once are only affected indirectly through the update value
    * N-step: Single reward of transition affects values of n preceding state action pairs. Makes process of propagating rewards to relevant state-action pairs much more efficient

* No use of replay memory - but: rely on parallel actors employing different exploration policies to perform stabilization - e.g. use different epsilons/schedules

* Async 1-step Q: At each step each agent computes grad of Q-loss. Use shared and slowly changing target network. Accumulate gradients over multiple timesteps before applying - similar to minibatches. Reduces chance of multiple actors overwriting each others updates. Accumulation also allows to tradeoff sample and computational efficiency. Also different exploration per thread where eps randomly sampled at intervals.

* Async 1-step Sarsa: Same as Q but with different target value - actually executed action in next step.

* Async n-step Q: Forward view by explicitly computing n-step returns. Authors found that this is easier when training Momentum-based and with backprop through time. Take tmax steps and compute gradients for n-step Q-Learning updates for each of the state-action pairs encountered since last update.
    * Each n-step update = longest possible n-step return - one-step update for last state, two-step update for second last update. Accumulated updates are applied in single gradient step.

* Async A2C: Policy and value estimate - forward view operation. k hyperparam for value estimate. Usually share params across actor and critic! Added entropy regularization.

* Performance scales with the number of workers - even superlinear speedups possible - often requires even less data = maybe due to reduction of bias in one-step methods

* Also seems to be robust to different learning rates as well as initializations.

#### Questions/Critical Observations:

* Hogwild! (Recht et al 2011): Communication of sending gradients and params across threads on single machine. Read!

* How do k, n and number of workers interact! Look at scalability as function of env. What does this tell us about complexity of problem?

* Possible combination with ER - Hindsight ER! and other advances - see Rainbow
