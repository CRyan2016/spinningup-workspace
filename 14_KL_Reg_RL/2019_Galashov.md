# Title: Information Asymmetry in KL-Regularized RL

# Author: Galashov et al. (2019; DeepMind)

#### General Content:

The authors provide an approach to leverage repeated structure in learning problems and propose to enforce learning of a default policy. This is done by restricting the state information received by the default policy (e.g., extero vs proprioceptive) and can lead to reusable behaviour is sparse reward environments. Since Mr Teh is on the paper :) they derive various connections to info bottleneck approaches as well as variational EM style algos.

#### Key take-away for own work:


#### Keypoints:

* Motivation: Bounded rationality - cost of info processing. Get rid of burden with the help of default behavior. Channel capacity constraints. Here: inject subjective knowledge by learning a default policy (relevant in multiple contexts) alongside agents policy.

* Info asymmetry: Prevent default to have access to certain state info. Hide parts of the state - force generalization!

* Starting with the kl-regularized exp reward objective - they rewrite the KL term using a default policy formulation $\pi_0(.|x_D)$
    * Relationship to probabilistic inference: choose default/prior as uniform to recover standard SAC entropy-reg objective
    * Learn $\pi_0$ end-to-end - hence goal-agnostic version = does not favor any particular behavior
    * Optimisation of objective wrt to default = supervised learning on trajectories generated by agent policy! - distillation process
    * Interpretation: default policy acts as a form of reward shaping + entropy in KL discourages deterministic policies

* Connection to info bottleneck approaches: Find that KL reg objective = lower bound on mutual info version
    * capacity constraint on channel between goal-dir history info and future actions
    * penalize dependence on info hidden from default policy

* Connection to Variational EM: autoencoder/reparam trick setup
    * KL objective is similar to p,q KL!
    * Interpret as a form learning a generative model of behaviors

* Implementation:
    - alternating GD between default and agent policy - keep other policy fixed
    - pi_0 optimisation = supervised learning given trajectories from pi
    - pi optimisation = reg expected reward problem
    - Continuous action space: SVG(0) with ER modification to SAC
    - Discrete action space: IMPALA, K-step returns + V-trace

* Experiments:
    - Continuous Control:
        * walkers with different DoF/actuators - different tasks (Walking, walls, target approach, move target, foraging)
        * especially strong speed up for sparse reward settings & default can structure the exploration/add substantial info
        * With dense rewards this might not be the case
        * Large KL values indicate strong behavioural changes
        * Default policies can be transfered between tasks and help!
        * Ablations show that gains possible when strong prior is known apriori
    - Discrete Action Space:
        * PBT - IMPALA concatenate past actions, rewards through residual net + lstm
        * DeepMind Lab experiments without constraining the action space (forward bias)
        * Default policy can provide an efficient filter to the action space

#### Questions/Critical Observations:
* Embodiment as a form of default policy - soft hand!
