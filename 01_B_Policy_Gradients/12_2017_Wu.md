# Title: Scalable trust-region method for DRL using Kronecker-factored approximation

# Author: Wu et al (2017) (Vector Institute)

#### General Content:
Introduce the ACKTR extension to A2C which uses a kronecker-factorized approximation to the trust region natural gradient. They further apply this update to the critic by assuming a Gaussian model/policy and noting the equivalence of this to Gauss-Newton (when using Fisher info matrix.)


#### Key take-away for own work:
Need to learn more about optimization. Read Nocedal & Wright Book!


#### Keypoints:

* Steepest descent = Minimize objective around perturbation of parameters constraint to update in parameters wrt to some metric.
    * SGD: Metric based on identity matrix/euclidean - problem depends directly on parametrization - arbitrary choice! should not affect optimization trajectory
    * Natural grad using fisher matrix instead - local quadratic approximation to the KL divergence - many parameters make exact inversion impractical - have to approx

* Use more advanced optimization techniques to overcome weakness of sample inefficiency. - **Natural gradient**: Follow steepest descent dir using Fisher metric - based on manifold/surface and not coordinates. Exact compuatation intractable because of inversion of Fisher matrix.
    * TRPO: avoids inversion with Fisher-vector products - typically requires many steps of conjugate gradient to obtain single parameter update (?) + large number of samples in batch to accurately estimate curvature. - Does not scale to large models!

* K-FAC: Kronecker-factored approximation to curvature. Scalable!
    * Keeps running average of curvature info - allows to use smaller batches
    * Complexity similar to plain SGD - easy to invert covariance of the gradient
    * For more notes on math - look at annotated paper/fisher/natural gradient notes
    * Approx. Block factorization Fisher in network when using backprop (result holds for MLPs/CNNs)
    * Underlying assumption: Second-order statistics of activations and backproped derivatives are uncorrelated
    * Can then easily write down grad wrt to weights as product of activation matrix inv x Jacobian wrt to weights x grad of output next layer wrt to preactivations inv
    * Only requires matrix computations of matrices with dims of W (in left and out right)

* Applying the natural gradient to the critic   
    * Learning critic: Regression problem with moving target - often done with 2nd order algo such as Gauss-Newton
    * Gauss-Newton matrix is equivalent to Fisher matrix for Gaussian observation model - allows us to apply K-FAC also here!
    * Use additionally Tikhonov damping - Martens and Grosse - what is this?

* Experiments: Show on MuJoCo and ATARI baselines improvements in terms of sample efficiency and computation (avg time steps per second) - problem: very hardware dependend


#### Questions/Critical Observations:
