# Title: Distilling Policy Distillation

# Author: Czarnecki et al. (2019: DeepMind)

#### General Content:
Compare different policy distillation techniques & their resulting objectives. They differentiate between teacher & student driven optimisation and introduce a new technique called expected entropy regularized distillation that seems promising. Everything is tabular though!

#### Key take-away for own work:


#### Keypoints:

* Knowledge transfer: Agent trained to match state-dependent probability distribution over actions provided by the teacher - different formulations:
    - trajectories sampled from teacher
    - trajectories sampled from student or mixture - often better performing
    - KL between teacher/student distributions or entire trajectories

* Contributions:
    1. Distillation with student trajectories does not for valid gradient field. Still has convergence guarantees but can oscillate as soon as rewards are introduced. Show methods how to recover gradient fields
    2. Empirical Evaluation of different control policies - when and why it is beneficial to use student-driven distillation
    3. AC setup where there is also access to teachers value function in addition to policy
    4. Propose new distillation variants + rule of thumbs

* Full objective: Combination of reward term from extrinsic/intrinsic components (long-term) & auxiliary loss for policy alignment at current timestep
    - Updates proportional to this objective - many different rules = not all have valid losses & valid gradient fields
    - Show that many rules do not have corresponding loss functions: Using student to generate MC samples for gradient

* Student distill (control policy rolled out for MC samples is student policy): Gradient operator is under expectation wrt same set of variables that it operates upon. Not clear if process leads to convergence. Classic RL problem!

* Theoretical results:
    - Given that student policy guarantees non-zero sampling each state visited by teacher - dynamics will converge. Easily satisfied by setting student to have softmax policy. "Strongly stochastic student policy"
    - If rewards are added dynamics can cycle and never converge.
    - Reward based correction can be added to ensure convergence - restore propere gradient field
    - Trade-off between convergence and fidelity of behaviour replication - control by functional form of control policy

* Relationship to compatibility of value function and policy! Which proves for AC setup that compatible value functions (same gradient as policy) provide convergence to the optimal policy.

* N-distill: Objective function which combines classic 1-step on-policy distillation with reward policy gradient term (log pi x l) - restores gradient field and leads to convergence.

* **Key Question**: Why does following the student policy when performing distillation typically lead to better empirical results?
    * Hypothesis: If convergent, student policy provides more robust policies wrt. trajectories sampled from student. This is due to more diverse sets of states visited
    * Key intuition: Training in regime that we expect to encounter during test time is key!
    * Furthermore, we continue training the distilled student on its own afterwards
    * Therefore: Expectation with respect to trajectories sampled from the students policy matters.

* Empirical Evaluation: MDPs - Gridworlds
    - If interested in teacher matching if student is allowed to generate transitions on its own - use student distillation. Benefits disappear as teacher is more uniform/entropic in its behavior.
    - Train vs test distribution: Case in which teacher driven distillation works better is when eval is done via expected KL under trajectories under trajectories generated from the teacher distribution

* Two approaches:
    1. Max probs of trajectory generated by student under the teacher policy.
    2. Learning = per-timestep supervised learning problem

+ Whose policy to use as a prior?
    - Entropy reg case: Use student and absorb cross-entropy between whole trajectories distributions into reward channel

* Expected Entropy Regularised Benefits:
    - Creates a valid gradient field
    - Does not suffer from high variance of the estimate
    - Directly maximises probability of student produced trajectories under the teacher policy as opposed to n-distill which looks at max the prob of being in states where the student and teacher agree.

* Actor-Critic Distillation setup
    - value critics allow to better leverage imperfect teachers - estimate how much to trust
    - Usefulness depends on quality of teacher and accuracy of value function estimators
    - Strength of 'state discriminatory signal' of critic has to be there!
    - Use critic as intrinsic reward/shaping
    - Experiments with adversarial teachers


#### Questions/Critical Observations:
