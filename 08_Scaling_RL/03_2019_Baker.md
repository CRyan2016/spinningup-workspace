# Title: Emergent Tool Use From Multi-Agent Autocurricula

# Author: Open AI et al. (2019)

#### General Content:
Simple multi-agent reward structures together with self-play can lead to self-supervised skill acquisition that is more efficient than intrinsic motivation. They show in a simple hide-and-seek setup the emergence of an autocurriculum and disctinct plateus/phases of dominant strategies. Standard OpenAI setup PPO + GAE + LSTM-based policy

#### Key take-away for own work:

* Think of exploration as a self-play objective. Winner who goes further.
* Centralization of critic appears to be key to provide solid signal.
* Use a buffer system and chunk full episode into 10 timestep chunks. "Replay" each sequence maximum amount of 4 times to keep on-policiness assured. 60 SGD steps in one optimization step/epoch.

#### Keypoints:

* Overcome hard design Qs - survival is simplistic. Natural curriculum in terms of self-play. Similar to locomotion by DeepMind.
    * Problem with current exploration methods: Bad scaling with environment complexity & different from evolutionary ideas of natural selection
    * "When a new successful strategy/mutation emerges, it changes the implicit task distribution neighboring agents need to solve and creates a new pressure for adaptation." - Leads to autocurriculum
    * Co-adaptation = here evidence for better scaling with env complexity and leads to human-interpretable behavior

* Develop "targeted" intelligence tests based on speed of adaptation from H&S pre-training
    * Transfer tries to tackle nature vs nurture debate

* Environment Hide-and-Seek Setup
    * 2 Teams: Hiders have to avoid line of sight from seekers
    * Objects that can be moved and locked
    * Hiders can use preparation phase (ca 100 steps out of 240)
    * Zero-sum game +/-1 reward, Make sure agents dont go to far out of env
    * 1-3 hiders, 1-3 seekers, 3-9 movable boxes, 2 movable rampls, static walls, rooms
    * Observations: position, velocity, size of objects and other agents
    * Mask out entities in policy if not in line-of-sight or in 135 cone in front
    * 30 range lidar sensors evenly around them spaced
    * Observe other team and how much time in preparation phase left
    * Agents spherical objects + move, grab, lock action space

* Agent setup: A3C + PPO + GAE + LSTM with Attention Architecture
    * CT-DE: Separate critic (no torso sharing) allows for full state input (no masking out) and better training signals
    * Use a buffer of previous agents and play against one of them in 5% of the cases - improves policy robustness
    * Entity centric observations * attention mechanism for object-level info
    * Use self-attention = permutation invariant and generalizes to varying number of entities

* Results: Pressure through multi-agent competition
    * 6 distinct phases of dominant strategies - shifts based on interaction with tools in the environment (robust across seeds)
    * Hiders learn a division of labor - due to team-based rewards
    * Large batch-sizes significantly stabilize learning. Less optimization steps required
    * Compare with Random Network Distillation baseline - simplistic autocurriculum outperforms

* Intelligence tests/transfer - problem of entangled skill representations
    * Memory tasks do gain from transfer, but manipulation tasks are uncorrelated.

#### Questions/Critical Observations:

* Read Leibo et al 19 a&b, Bansal et al 18, Liu et al 19
