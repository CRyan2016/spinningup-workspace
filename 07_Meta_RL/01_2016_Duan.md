# Title: RL^2 Fast RL via Slow RL

# Author: Duan et al. (2016, OpenAI)

#### General Content:
First paper to introduce the new era of Meta-RL via training an RNN on multiple different envs sampled from an overlapping task distribution. Framing is done very much from a Bayesian PoV of building a good prior via integration of info.


#### Key take-away for own work:
View the learning process of the agent itself as an objective. And average objective over distribution of MDPs - Prior distilled into agent. Main difference to Wang - Hidden state is preserved across episodes. Cast learning an RL algo as an RL problem.

#### Keypoints:

* Main problem of RL: Sample complexity - overcome by two scales/loops of learning

* Idea: Recurrent Weights of RNN can encode a separate learning algorithm - dynamics! The characterizing weights are learned slowly via SGD in outer loop while inner dynamics act fast.

* Traditional bayesian approaches to RL have failed since they scale poorly and need to make assumptions regarding the env - here: Learn the dynamics

* Prototype a first example with Multi-Arm bandits as well as a vision-based navigation task (VizDoom - need to learn a map - small 5x5, 9x9 mazes)

* RNN feed in state, prev action/reward, termination - use GRU based architecture + TRPO optimization with GAE baseline correction

#### Questions/Critical Observations:

* Comparison to Wang et al - Consider more arms and more episodes per bandit
