# Title: Dueling Network Architectures for DRL

# Author: Wang et al 2016 (DeepMind)

#### General Content:

Introduce the dueling architecture for DQNs which split FFW net architecture into two streams for state value (1 hidden layer part + 1 unit) and (1 hidden layer part  + num-actions units). Combination of both streams via Q = V + A - max A or Q = V + A - 1/|A| sum A. Allows the network to learn better policy eval when there are many "val-equivalent" actions. Furthermore, there is a value stream update each time we perform a SGD step vs only one action value update in single stream architectures. Advantage of architecture stronger with more actions.


#### Key take-away for own work:

Go back to old ideas and revise - initial work by Baird (1993 - advantage updating). Target obvious inefficiencies and think about sample efficiency. Very incremental progress - allows for good baseline improvements.

Main Intuition: For many states estimating value for each action does not matter! For bootstrapping based algos state value estimate is always important.

#### Keypoints:

* Explicit focus on changing up the architecture of the network for model-free RL. SOTA for ATARI benchmark together with DDQN, PER and grad clipping.

* Architecture can learn valuable states without sample inefficient pathway the effect of actions for each state.

* Visualisation of usefulness with Jacobian of Value and Action representations wrt input image shows what the individual streams "listen"/are sensitive to.

* Double DQN: DQN max operator in target/bootstrap estimate uses same values to select and evaluate an action - overoptimistic value estimates. Move max Q to argmax Q inside the bootstrap estimate.

* Prioritzed ER Intuition: replay tuples with expected learning progress (absolute TD error) more often. Dueling increases for both uniform sampling as well as prioritized sweeping version.

* Dueling Architecture:  
    1. In: (84x84x4)
    2. 1Hidden: 2D Conv 64 8x8 filters + Stride 4 + ReLU
    3. 2Hidden: 2D Conv 32 4x4 + Stride 2 + ReLU
    4. 3Hidden: 2D Conv 32 3x3 + Stride 1 + ReLU
    5. Fully Connected: 512 Units Value | 512 Units Action + ReLU
    6. Fully Connected: 1 Unit Value | Num Actions Units (Advantages)
    7. Out: V + (A - max A) or V + (A - avg A)

* Other hyperparams as in Hasselt and priority exp 0.7 and importance sampling annealing schedule from 0.5 to 1

* Mean advantage zero since Q(s,a) average over policy is equal to V(s). Also A(s, a^star) = 0 for deterministic policy

* Cant simply do Q = V + A since Q is parametrized estimate and not true Q
    * V(alpha) and A(beta) not necessarily a good estimate and from Q cant uniquely invert to exact decomposition.
    * Enforce advantage estimator to have 0 advantage at chosen action by subtracting max over a from A.
    * V + (A - max A) vs V + (A - avg A): Loose interpretation of A,V with avg since off by constant -(max A - avg A) - benefit of increased stability since mean changes slower than max. Also: Mean preserves ranking of values and action choice. No direct effect on behavior.

* ATARI Hacks:
    * Two gradients flow into feature detector layers (one from each stream) - need to rescale the combined gradient by 1/sqrt(2)
    * Gradient clipping <= 10 commonly done in RNNs
    * Again this weird human baseline computation: pick random starting positions after 30 random actions


#### Questions/Critical Observations:

* DQN should refer more to the way how the architecture is trained (ER + SGD + Clipping + Preproc, etc.) rather than the architecture itself. That can be trained in many diffferent ways - i.e. SARSA
* A(s, a^star) = 0 hack/enforcement comes for case of deterministic policy. What if this is not given?
* Interactions between extensions: Grad clipping + PER - Sampling transitions with higher TD error leads to gradients with higher norms - retuning of learning rate!
