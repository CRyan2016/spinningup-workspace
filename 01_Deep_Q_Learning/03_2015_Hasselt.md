# Title: Deep Reinforcement Learning with Double Q-Learning

# Author: van Hasselt et al (2015) (DeepMind)

#### General Content:

Generalize original double Q-Learning (van Hasselt 2010) to DQNs. Authors show that Q-Learning is overoptimistic due to any form of estimation error (not only noise or flexibility). This is due to the max operation being used for eval and selection - results in preference for overestimated rather than underestimated values. They propose to decouple these steps: different action value function to select (value network) an action and to evaluate (target network) the actions. This naturally fits into DQN target-value network procedure.


#### Key take-away for own work:

"Overoptimism in the face of apparent certainty." Again the idea was already out there 5 years ago and the intuition remained valid. Also had the objective to improve upon DQN with minimal change to the actual pipeline.

#### Keypoints:

* Q-Learning incldues max step over est of action values - prefers overestimated values. Constant/Uniform bias would not affect quality of policy. And could also lead to better exploration ala UCB.

* UCB type effect would only result if the overestimation was concentrated at part of space which we want to learn about.

* DQN - Deterministic env -> no noise factor for overest. Still Double DQN improves significantly (even when simply using DQN params)

* General Double Q bootstrap estimate: $Q(s', argmax Q(s', a'; \theta), \theta')$ - In DQN setting conveniently $\theta' = \theta^-$

* Give nice and simple 1d continuous state space example with varying flexibility in function approx (polynomial degrees) and different true value functions - samples match true function exactly - form of GP excercise
    * Shows that overestimation also occurs if there is access to exact values at certain state space points.
    * 10 discrete actions which all have same value - not exact values doe to approx for 11 points with 9 degrees

* DQN Experimental Evaluation:
    * Nair et al (2015) - Evaluate the algo based on random starts generated by human expert - otherwise argument of remebering sequences
    * 30 No-op starts: Start eval after 30 random actions
    * For DQN keep 0.05 as exploration value during evaluation

* Show that DDQN does not only have better value estimates but also better policies!
    * True value estimation: Rollout after learning finished and compute cumulative rewards - without overestimation: curves should end up same.

* Hacks specific to tuned version of DDQN:
    * Increased number of frames between update of target network from 10000 to 30000 - Since observation: directly after switch both DQN and DDQN go back to simple Q Learning
    * Different exploration schedule - 0.1 tp 0.01 linear decay during learning, 0.001 during evaluation
    * Sharing of single bias for all action values on top layer of net

#### Questions/Critical Observations:

* Reimplement the easy 1d example

* Look at propagation of overestimation
